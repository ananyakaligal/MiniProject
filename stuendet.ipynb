{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Data Preparation (Simulated)\n",
    "\n",
    "# Example: Simulated Dataset (Replace with real student dataset)\n",
    "num_samples = 2000  # Number of students\n",
    "num_features = 20  # Example: 20 input features\n",
    "\n",
    "# Randomly generated student features\n",
    "X_train = np.random.rand(num_samples, num_features)\n",
    "X_test = np.random.rand(500, num_features)\n",
    "\n",
    "# Regression target: Final grades (0-100)\n",
    "y_grade_train = np.random.rand(num_samples, 1) * 100\n",
    "y_grade_test = np.random.rand(500, 1) * 100\n",
    "\n",
    "# Classification target: At-risk students (0 or 1)\n",
    "y_risk_train = np.random.randint(0, 2, (num_samples, 1))\n",
    "y_risk_test = np.random.randint(0, 2, (500, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Model Definition (Shared Layers + Branching)\n",
    "\n",
    "# Define Input Shape\n",
    "input_shape = (num_features,)\n",
    "inputs = Input(shape=input_shape)\n",
    "\n",
    "# Shared Layers (Feature Extraction)\n",
    "x = Dense(512, activation='relu')(inputs)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)  # Regularization\n",
    "\n",
    "# Task 1: Final Grade Prediction (Regression)\n",
    "grade_branch = Dense(64, activation='relu')(x)\n",
    "grade_branch = Dropout(0.2)(grade_branch)\n",
    "grade_branch = Dense(32, activation='relu')(grade_branch)\n",
    "grade_output = Dense(1, activation='linear', name='grade_output')(grade_branch)\n",
    "\n",
    "# Task 2: At-Risk Classification (Binary Classification)\n",
    "risk_branch = Dense(64, activation='relu')(x)\n",
    "risk_branch = Dropout(0.2)(risk_branch)\n",
    "risk_branch = Dense(32, activation='relu')(risk_branch)\n",
    "risk_output = Dense(1, activation='sigmoid', name='risk_output')(risk_branch)\n",
    "\n",
    "# Define the Multi-Task Model\n",
    "model = Model(inputs=inputs, outputs=[grade_output, risk_output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Compile Model with Multi-Task Loss\n",
    "\n",
    "\n",
    "# Custom loss weights: Adjust task importance (1.0 means equal priority)\n",
    "loss_weights = {'grade_output': 1.0, 'risk_output': 0.5}\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss={\n",
    "        'grade_output': 'mse',  # Mean Squared Error for Regression\n",
    "        'risk_output': 'binary_crossentropy'  # Binary Cross-Entropy for Classification\n",
    "    },\n",
    "    loss_weights=loss_weights,\n",
    "    metrics={\n",
    "        'grade_output': 'mae',  # Mean Absolute Error\n",
    "        'risk_output': 'accuracy'  # Classification Accuracy\n",
    "    }\n",
    ")\n",
    "\n",
    "# Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Train the Model\n",
    "\n",
    "# Callbacks for better training\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    {'grade_output': y_grade_train, 'risk_output': y_risk_train},\n",
    "    validation_data=(X_test, {'grade_output': y_grade_test, 'risk_output': y_risk_test}),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 5: Evaluate the Model\n",
    "\n",
    "results = model.evaluate(X_test, {'grade_output': y_grade_test, 'risk_output': y_risk_test})\n",
    "print(\"\\nTest Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 6: Make Predictions (Custom Dummy Data)\n",
    "\n",
    "\n",
    "# Example: Custom test student data (Replace with real values)\n",
    "sample_students = np.array([\n",
    "    [0.8, 0.6, 0.5, 0.7, 0.9, 0.3, 0.4, 0.2, 0.8, 0.9, 0.1, 0.7, 0.6, 0.3, 0.5, 0.8, 0.9, 0.6, 0.4, 0.7],  # Student 1\n",
    "    [0.4, 0.5, 0.2, 0.6, 0.3, 0.8, 0.9, 0.7, 0.1, 0.4, 0.6, 0.3, 0.5, 0.7, 0.9, 0.1, 0.2, 0.8, 0.9, 0.5]   # Student 2\n",
    "])\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(sample_students)\n",
    "\n",
    "# Print Results\n",
    "for i in range(len(sample_students)):\n",
    "    print(f\"\\nStudent {i+1}:\")\n",
    "    print(f\"   Predicted Final Grade: {predictions[0][i][0]:.2f}\")\n",
    "    print(f\"   At-Risk Probability: {predictions[1][i][0]:.2f} (Threshold 0.5)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Plot Training History\n",
    "\n",
    "\n",
    "def plot_training(history):\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Loss Plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['grade_output_loss'], label='Grade Loss')\n",
    "    plt.plot(history.history['risk_output_loss'], label='Risk Loss')\n",
    "    plt.plot(history.history['val_grade_output_loss'], label='Val Grade Loss', linestyle='dashed')\n",
    "    plt.plot(history.history['val_risk_output_loss'], label='Val Risk Loss', linestyle='dashed')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy Plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['risk_output_accuracy'], label='Risk Accuracy')\n",
    "    plt.plot(history.history['val_risk_output_accuracy'], label='Val Risk Accuracy', linestyle='dashed')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Classification Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_training(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
